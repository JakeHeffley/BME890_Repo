{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Notes: Support Vector Machine\n",
    " - good for small to medium sized datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    " - \n",
    "    - attempts to draw a straight line between classes and stay as far away from the closest training instances as possible\n",
    "    - aka margin classification\n",
    "    - Support Vectors are the training instances which are nearest the margin\n",
    " - Soft Margin Classification\n",
    "    - if you do not allow exceptions of a few instances on the wrong side of you margin then you could draw a very bad margin or be unable to draw a margin at all (hard margin classification)\n",
    "    - Balance between making the margin between classes as wide as possible vs having as few margin violations as possible. \n",
    "    - hyperparameter C in sklearn SVM class sets the number of margin violations. Low C means more violations. \n",
    "    - Accepting more margin violations may make the model more generalizable. \n",
    "    - linearSVC needs the mean removed (standarScaler) to perform optimally\n",
    "    - loss function should be hinge and the hyperparameter dual should be false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification \n",
    " - Using a nonlinear transformation can sometimes make the dataset linearly separable\n",
    "     - adding polynomial features is one example \n",
    "     - adds new features (dimensions) to the dataset\n",
    "     - PolynomialFeatures in sklearn does this well \n",
    " - Polynomial Kernel \n",
    "     - adding too many polynomial features can over complicate the dataset\n",
    "     - The Kernel Trick gets you the benefits of a high degree polymial without having to calculate the whole thing\n",
    "     - You can try modifying the degree of the polynomial in order to account for over or underfitting\n",
    " - Similarity Features\n",
    "     - A similarity function measure how much each instance resembles a \"landmark\"\n",
    "     - Gaussian Radial Basis Function (RBF) can be used as a similarity function. Makes a gaussian distribution around the landmark. \n",
    "     - You then replace the original features with the similarity scores for each instance relative to each landmark\n",
    "     - optimally you would set a landmark at the location of each and every instance in the dataset. But this may be too computationally expensive. \n",
    " - Gaussian RBF Kernel\n",
    "     - method of using the kernel trick to reduce the computational complexity for using similarity features. \n",
    "     - they perform SVC with the gaussian rbf kernel. You can modulate gamma and C to regulate the classfier.\n",
    "         - if the model is overfitting you can reduce gamma and vice versa\n",
    " - Computational Complexity\n",
    "     - LinearSVC training time scales linearly ( O(m * n) )\n",
    "     - SVC class scales exponentially with the number of instances. So it is best for small or medium datsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    " - conceptually, svm regression attempts to find a street between two classes with as many samples on the street as possible while limiting margin violations. \n",
    " - epsilon - hyperparameter controlling width of the street\n",
    " - nonlinear svm regression requires use of a kernal like a polynomial kernel\n",
    " - SVMs can also be used for outlier detection. Not covered here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    " - \n",
    "    - in this chapter the bias term will be separated from the model parameters (b and w respectively) instead of lumped together in theta\n",
    " - Decision Function and Predictions\n",
    "     - Decision Function (h) - Sum the multiple in of the parameters and input features:  $\\hat{y}$ = $\\textbf{w}^T\\textbf{x}$ + b\n",
    "     - positive results are one class (1) and negative results are the other (0). \n",
    "     - Training an svm classifier means finding the values of w and b which make the margin between classes as wide as possible while avoiding or limiting margin violations\n",
    " - Training Objective\n",
    "     - the slope of the decision func is the norm of the weights h =  ||$\\textbf{w}$||\n",
    "     - $\\textbf{w}$ and the size of the margin are inversly proportional\n",
    "     - So in a way the svm projects the data as a N dim plane into an N+1 dim space. The extra dim is the value for the decision function, positive or negative. The decision function itself is an N dim plane separating the two classes. The two ends of the margin or \"street\" are the portions in the data plane where the value of the decision function =1 or -1. \n",
    "     - define $\\textbf{t}^{(i)}$=-1 for $\\hat{y}$ =0   and   $\\textbf{i}^T$=1 for $\\hat{y}$ =1\n",
    "     - constraint $\\textbf{t}^{(i)}$($\\textbf{w}^T\\textbf{x}^{(i)}$ + b) $\\geq$ 1  for all instances\n",
    "     - the decision function is $\\textbf{w}^T\\textbf{x}^{(i)}$\n",
    "     - Hard Margin linear SVM classifier objective: minimize$_{w,b}$  $\\frac{1}{2}$$\\textbf{w}^T\\textbf{w}$ as constrained by  $\\textbf{t}^{(i)}$($\\textbf{w}^T\\textbf{x}^{(i)}$ + b) $\\geq$ 1 for all values of i = 1:m\n",
    "     - Soft Margin Objective:\n",
    "     - slack variable $\\zeta$ - for each instance zeta measures how much that instance can violate the margin\n",
    "     - trade off between minimizing the slack variables  and minimizing  $\\frac{1}{2}$$\\textbf{w}^T\\textbf{w}$\n",
    "     - C hyperparameter defines this trade off\n",
    "     - minimize$_{w,b}$  $\\frac{1}{2}$$\\textbf{w}^T\\textbf{w}$ C * sum $\\zeta^{(i)}$   constrained by $\\textbf{t}^{(i)}$($\\textbf{w}^T\\textbf{x}^{(i)}$ + b) $\\geq$ 1 - $\\zeta^{(i)}$   with  $\\zeta^{(i)}$$\\geq$ 1\n",
    " - Quardratic Programming\n",
    "     - QP are convex optimization problems with linear constraints. This includes soft and hard margin classification\n",
    "     - an off the shelf QP solver will output $\\textbf{x}$ which is a vector consisting of a bias term and n weights for the features. \n",
    " - The Dual Problem\n",
    "     - Primal Problem - a constrained optimization problem that appears in the SVM problem. It is hard to solve. \n",
    "     - Dual Problem - much easier to solve than the primal problem but under the conditions of hte SVM problem it has the same solution as the primal problem. \n",
    "     - see equation 5-6 for the dual form of the linear SVM objective. Use a QP solver to find the vector $\\hat{\\textbf{a}}$ which minimizes the equation. \n",
    "     -  $\\hat{\\textbf{a}}$ allows you to solve the primal equation by calculating $\\hat{\\textbf{w}}$ and $\\hat{b}$ (equation 5-7)\n",
    " - Kernelized SVMs \n",
    "     - apply 2nd degree polynomial transform to 2D dataset then train a linear SVM on it. \n",
    "     - $\\phi$ = 2nd degree polynomial mapping funciton\n",
    "     - $\\phi$($\\textbf{x}$) = $\\phi$($x_1, x_2$) = ($x_1^2, \\sqrt{2}x_1x_2, x_2^2$)\n",
    "     - No apply the mapping function to your two feature vectors $\\textbf{a}$ and $\\textbf{b}$\n",
    "     - $\\phi$($\\textbf{a}^T$)$\\phi$($\\textbf{b}$) = ($\\textbf{a}^T$$\\textbf{b}$)$^2$\n",
    "     - if you know the kernel for the transformation you want to perform then you can often perform a much more computationally efficient shortcut like the one showed above. \n",
    "     - the function K($\\textbf{a}$, $\\textbf{b}$) = ($\\textbf{a}^T$$\\textbf{b}$)$^2$ is a 2nd degree polynomial kernel\n",
    "     - Kernel - in ML a kernel is a function capable of computing the dot product of the transformed features using only the original vectors $\\textbf{a}$ and $\\textbf{b}$. This means you can skip the process of computing the transformation (in this case $\\phi$).\n",
    "     - There are kernels for linear, polynomial, gaussian rbf, and sigmoid transformations\n",
    " - Online SVMs\n",
    "     - when implementing SVM classifiers online SGD is often used to minimize the cost function. \n",
    "     - the cost funtion is derived from the primal problem. \n",
    "     - J($\\textbf{w}$,b) = $\\frac{1}{2}$$\\textbf{w}^T\\textbf{w}$ + C$\\sum_{i=1}^m$max(0,1-t$^{(i)}$($\\textbf{w}^T\\textbf{x}$+b))\n",
    "     - the first term will push the model to have small weights $\\textbf{w}$ resulting in a larger margin. \n",
    "     - the second term will computes the total of all margin violations. Minimizing this reduces the size and number of margin violations. \n",
    " - Hinge Loss \n",
    "     - max(0,1-t)\n",
    "     - slope = -1 for t<1\n",
    "     - slope = 0 for t>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
