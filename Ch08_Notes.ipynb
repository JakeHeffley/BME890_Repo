{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8508b0ec",
   "metadata": {},
   "source": [
    "# Chapter 8 Notes: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f55911",
   "metadata": {},
   "source": [
    "## Intro\n",
    "- usually there is a trade-off, dimensionality reduction speeds up training but reduces performance. \n",
    "- sometimes dim red will eliminate noise and result in an increase in performance.\n",
    "- useful for data visualization and avoiding the curse of dimensionality.\n",
    "- two main methods: projection and manifold learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481cae0",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    " - In high dimensional spaces almost all points will be \"extreme\" values. \n",
    "     - i.e. they lie near the borders of the state space\n",
    " - Additionally, randomly points in a high dimensionality space will be much farther from each other compared to points in a lower dimensional space. This has two drawbacks:\n",
    "     - It is much harder to sample a high dimensional space and requires far more training data\n",
    "     - the relative variance of the distance between points within the space is much reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb07b70",
   "metadata": {},
   "source": [
    "## The Main Approaches for Dimensionality Reduction\n",
    "### Projection\n",
    " - In real world data the training instances are not distributed evenly across all dimensions\n",
    "     - some dimensions will have little variance\n",
    "     - others will be highly correlated with other dimensions\n",
    " - the data may be restricted to a smaller dimensionality subspace of the original state space\n",
    " \n",
    "### Manifold Learning\n",
    " - if the lower dimensional subspace is twisted or bent within the original higher dimensional space then we call this a manifold. \n",
    "     - projection will squash the manifold and not produce a good output\n",
    " - For example, the swiss roll is a 2D plane twisted in the 3rd dimension. \n",
    " - Manifold Assumption - most real world datasets lie close to a much lower dim manifold. \n",
    "     - e.g. if you were to randomly generate images a vanishingly small number would look like digits. So the dimensionality of the MNIST dataset is restricted to some manifold. \n",
    " - Often times reducing the dimensionality will simplify the decision boundary for an ML algorithm such as regression. But this is not always true. Sometimes the decision boundary is simpler in higher dims. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64075",
   "metadata": {},
   "source": [
    "## PCA \n",
    " - projects the data onto a smaller dimensional hyperplane which lies closest to the data. \n",
    " \n",
    "### Preserving the Variance\n",
    " - To choose a hyperplane to project the data onto, PCA identifies the hyperplane which minimizes the squared error between the points in the original data and those projected onto the hyperplane. \n",
    " - The preserves the largest amount of variance in the data\n",
    " \n",
    "### Principal Components\n",
    "  - PCA finds the axis which accounts for the maximum amount of variance in the data, then it searches the axes orthogonal to that one which accounts for the maximum amount of variance. It repeats this process for each dimension of the data. \n",
    "  - Each identified axis is called a principal component. This is represented by a unit vector which lies on the identified axis. \n",
    "  - Singular Value Decomposition (SVD) - decomposes the training matrix __X__ into three matrices __U $\\Sigma$ $V^T$__\n",
    "  - __V__ contains all the unit vectors of the PCs\n",
    "  - PCA requires that the mean of the data be centered around the origin. \n",
    "  \n",
    "### Projecting Down to d Dimensions\n",
    " - __$X_{d-proj}$__ = __X$W_d$__\n",
    " - Where __$X_{d-proj}$__ is the reduced dimensionality for of __X__ and __$W_d$__ is the first d columns of __V__\n",
    " - __X__ is m by n, __V__ is n by n, __$W_d$__ is n by d. Output of __X$W_d$__ is m by d\n",
    " \n",
    "### Explained Variance Ratio\n",
    " - The proportion of variance which lies along each PC's axis\n",
    " \n",
    "### Choosing the Right Number of Dimensions\n",
    " - One common tactic is to keep enough dimensions to explain 95% of the variance.\n",
    " - in sklearn's pca, setting n_components to a value betwee 0 and 1 will do this automatically. \n",
    " - Alternatively you can plot the cumulative sum of explained variance as you increase the d and look for an elbow. \n",
    " \n",
    "### PCA for Compression\n",
    " - For MNIST, retaining 95% of the variance reduces the features from 784 to 150.\n",
    " - Reconstruction Error - the mean squared distance between the original data and the data you get when you compress then decompress the data. In sklearn this is done with the inverse_transform method\n",
    " - inverse transform equation: __$X_{recovered}$__ = __$X_{d-proj}W_d^T$__\n",
    " - __$X_{d-proj}$__ is m by d, __$W_d^T$__ is d by n, the output will be m by n\n",
    " \n",
    "### Randomized PCA \n",
    " - much fast that full PCA when d is much smaller than n. \n",
    " - rapidly finds an approximation of d PCs\n",
    " \n",
    "### Incremental PCA\n",
    " - Offers batch processing for PCA.\n",
    " - requires calling the partial_fit() method on each mini-batch instead of fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc035793",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    " - Can use kernels (linear, RBF, sigmoid, etc) to project the data into a higher dimension before projecting it down to d dimensions. \n",
    " - This can help preserve clusters of instances in lower dimensional projections. \n",
    " \n",
    "### Selecting a Kernel and Tuning Hyperparameters\n",
    " - use the performance of a supervised learning algorithm on the output of PCA in order to determine the best hyperparameters for PCA. \n",
    " - Alternatively you can select the hyperparams which yield the lowest reconstruction error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874fed1",
   "metadata": {},
   "source": [
    "## Local Linear Embedding (LLE)\n",
    " - nonlinear \n",
    " - manifold technique which does not rely on projections\n",
    " - measures how each training instance relates to its closest neighbors (cn)\n",
    " - looks for a low dim representation where these local linear relationships are preserved. \n",
    " - For each training instance __x$^i$__ , LLE identifies its k closest neighbors and approximates __x$^i$__ as a linear function of those k instances\n",
    " - seeks to minimize the squared distnace between __x$^i$__ and $\\sum_{j=1}^{m} w_{i,j}x^j$\n",
    " - if $x^j$ is not one of the k closest neighbors then the weights are set to 0\n",
    " - produces a weights matrix __W__\n",
    " - The second step is to project the instances into a d dimensional space while preserving as much of the local relationships as possible. \n",
    " - __z__^i is the image of __x__^i in d dimensional space\n",
    " - therefore minimmize the distance between __z__$^i$ and $\\sum_{j=1}^{m} \\widehat{w}_{i,j}z^j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0820ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
