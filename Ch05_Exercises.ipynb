{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the fundamental idea behind support vector machines? \n",
    " - SVMs aim to create the widest possible street or margin between two groups. They use the data points from each group which are closest to the other group (support vectors) to define this margin.\n",
    " - Goal is to have the widest possible margin between the decision boundary and the training instances. \n",
    " - Also important to use kernels for training on nonlinear datasets. \n",
    "2. What is a support vector? \n",
    " - support vectors are the data points at the boundary between two groups of data. They are used to define the border between the two groups and calculate predictions. \n",
    " - Instances which are not on the margin do not affect the training or predictions. \n",
    "3. Why is it important to scale the inputs when using SVMs?\n",
    " - SVMs ultimately rely upon measuring the distance between points in order to create the widest possible margin between groups. Having features of vastly different scales distorts the distances between points and overweights the feature with the larger scale. \n",
    "4. Can an SVM output a confidence score when it classifies an instance? What about a probability? \n",
    " - The output of the decision function or the distance between an instance and the decision boundary can be used as a confidence score. SVMs do not output a probability for each class. \n",
    " - Using logisitic regression and cross validation, sklearn can calculate a probability for each instance when using an SVM. \n",
    "5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features? \n",
    " - You should use the primal form. The dual form is faster when the number of features is greater than the number of instances. \n",
    " - nonlinear (kernelized) SVMs can only use the dual form. \n",
    " - computational complexity for the primal form is directly proportional to the number of instances m, while for the dual form it is an exponential relationship, $m^2$ or $m^3$\n",
    "6. Say you've trained an SMV classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease gamma? What about C?\n",
    " - Increase gamma to decrease each instances range of influence on the decision boundary. This will introduce more variation in the decision boundary and reduce underfitting. Alternatively you could also increase C. \n",
    "7. How should you set the QP parameters to solve the soft margin linear SVM classifier problem using an off the shelf QP solver? \n",
    " - H an identity matrix of size 2m by 2m + 1 where m equals the number of instances. Top left value is zero to ignore the bias term. \n",
    " - f a vector of zeros with length = # features + 1 for bias term and concatenated with a vector with each element = C of length = #instances\n",
    " - A a matrix of size #constraints x #features+1 plus an mxm identity matrix to the right, an inverse identity matrix below that, and zeros in the remainder of the array. \n",
    " - b is a vector of -1s of length equal to the number of training instances concatenated with a vector of 0s equal in length to #instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train a linear SVC on a linearly separable dataset. \n",
    "#Then train an SVC and a SGDClassifier on the same dataset. \n",
    "#See if you can get them to produce roughly the same model. \n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, \n",
    "# you will need to use one versus the rest to classifiy all 10 digits.\n",
    "# Tune the hyperparameters using small validation sets to speed up the process. \n",
    "# What accuracy can you reach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Train an SVM Regressor on the California Houseing dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
