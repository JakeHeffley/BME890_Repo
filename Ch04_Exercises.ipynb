{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Exercises - Training Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which Linear Regression training algorithm can you use if you have a training set with millions of features? \n",
    "    - Gradient descent with lasso regression\n",
    "\n",
    "2. Suppose the features in your training set have very different scales. Which algorithm might suffer from this and how? What can you do about it? \n",
    "    - Gradient descent will be slowed since it will preferentially follow the gradient of the feature with the larger scale first instead of taking a direct route to the minimum. \n",
    "    - feature scaling\n",
    "\n",
    "3. Can Gradient Descent get stuck in a local minimum when training a logistic regression model? \n",
    "    - no, the cost function is convex\n",
    "\n",
    "4. Do all gradient descent algorithms lead to the same model, provided you let them run long enough? \n",
    "    - if the learning rate is too large then an algorithm could fail to converge. \n",
    "    - if there are local minima in the cost function then one GD may get caught in the local minima while another algorithm may succesfully escape it and converge on the global mimimum or a different local minima. \n",
    "    \n",
    "5. Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice the validation error consistently goes up, what is likely going on? How can you fix this? \n",
    "    - The model is getting over trained. \n",
    "    - Rollback to the model parameters at the minimum value for the validation error\n",
    "    - use a simpler model\n",
    "    - use a regularization technique\n",
    "    - use a larger training set to reach a lower error value before overfitting occurs. \n",
    "    - If the learning rate is too high then the validation and training error will both go up.\n",
    "    \n",
    "6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up? \n",
    "    - no, in mini-batch gd the validation error will have some noise/variance from epoch to epoch. So you need to wait to see that it is consistently rising then roll back to the minimum. \n",
    "    \n",
    "7. Which gradient descent alogrithm discussed will reach the viscinity of the optimal solution the fastest? Which will actually convege? How can you make others converge as well? \n",
    "    - Stochastic gradient descent will reach the right area fastest since it trains the fastest.\n",
    "    - Batch gradient descent will converge on the actual optimal solution eventually\n",
    "    - applying a good learning schedule to modulate the learning rate over the course of training can allow stochastic and mini-batch GD to converge. \n",
    "    \n",
    "8. Suppose you are using polynomial regression. You plot the learning curves and you notice there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "    - the gap means that the model is performing better on the training data vs the validation data. This means the model is overfitting the to the training data. \n",
    "    - You could use a simpler model such as linear regression or reduce the number of degress in your polynomial regression. \n",
    "    - You could regulate the cost function with ridge regression or lasso. Will reduce complexity by eliminating weights of unimportant features.  \n",
    "    - You could provide a large training dataset to allow the model to train longer before it becomes over fit. \n",
    "    \n",
    "9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter alpha or reduce it? \n",
    "    - the model has high bias since it seems to generalize well to the validation data but have poor performance. \n",
    "    - you should reduce alpha to trend more towards variance in the bias variance trade off. This will necessarily increase the variance, but if you manage to reduce the error enough before the variance rises to an unacceptable level then you may fix the problem. \n",
    "    \n",
    "10. Why would you want to use:\n",
    "    - Ridge Regression? Ridge regression will help to reduce the model's variance and generalize better to new data. It prevents overfitting.\n",
    "    - Lasso instead of Ridge Regression? Lasso regression is useful if you suspect that many of your features are not important. Lasso will reduce the weights of unimportant features to zero or near zero. \n",
    "    - Elastic Net instead of Lasso?  If you have more features than instances in your training data then you should use elastic net\n",
    "    \n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nightime. Should you implement two logistic regression classifiers or one softmax regression classifier? \n",
    "    - Since each picture will have up to two classes represented (daytime and outdoor or nightime and inside in the same pic) you should use two logistic regression classifiers. Although telling night from day indoors seems like it might be impossible any room without windows. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  12. Implement Batch Gradient Descent with early stopping for softmax regression without using scikitlearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = 10 * np.random.rand(1000,4)\n",
    "y = np.round(np.random.rand(1000,1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b_train = X_b[0:800,:]\n",
    "X_b_val = X_b[800:1000,:]\n",
    "y_train = y[0:800,:]\n",
    "y_val = y[800:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch GD\n",
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000\n",
    "m =1000\n",
    "\n",
    "theta = np.random.rand(4,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "#compute the linear regression prediction\n",
    "#parameter vector for each class\n",
    "#enter scores into softmax function\n",
    "# use argmax to make softmax predicion\n",
    "# cross entropy cost function\n",
    "#early stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
