{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93f37ed",
   "metadata": {},
   "source": [
    "# Chapter 10 Notes: Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fb34d",
   "metadata": {},
   "source": [
    "# From Biological to Artificial Neurons\n",
    " - McCulloch Pitts 1943 computational paper\n",
    "## Logical Computatiosn with Neurons\n",
    " - Uses artificial binary neurons\n",
    " - Can perform basic logic functions like AND, OR, =, OR NOT \n",
    " - Cannot do XOR\n",
    "## The Perceptron \n",
    " - Frank Rosenblatt 1957\n",
    " - Threshold Logic Units (TLU) make up the perceptron\n",
    " - inputs and outputs are scalars\n",
    " - each input is associated with a weight\n",
    " - each TLU computes a weighted sum of its inputs\n",
    "    - z = $w_1x_1$ + $w_2x_2$ ... = **$x^T$w**\n",
    " - Then the TLU applies a step function to the result and outputs the result\n",
    "    - $h_w$(**x**) = step(z) where z = **$x^T$w**\n",
    " - Types of step functions for Preceptrons:\n",
    "    - Heaviside Step Function - 0 until z > 0, then 1\n",
    "    - Sign Step Function - -1 for z < 0,  0 for z=0, 1 for z>0\n",
    " - A single TLU can perform linear binary classification. \n",
    " - A perceptron is merely a single layer of TLUs and an input layer \n",
    " - The input layer also contains a bias neuron which always outputs 1\n",
    " - Perceptrons can do multi-class classification\n",
    " - Computing the outputs of a fully connected perceptron layer:\n",
    "    - $h_{W,b}$ = $\\phi$(**XW** + **b**)\n",
    "    - **X**  # instances by # features\n",
    "    - **W** wieght matrix, # input neurons by # artificial neurons (TLUs) \n",
    "    - **b** bias vector contains weights between bias neuron and all the TLUs. len= # TLUs\n",
    "    - $\\phi$ activation function \n",
    " - Learning Rule: reinforces the connections which help reduce the error\n",
    " - $w_{i,j}^{(next Step)}$ = w$_{i,j}$ + $\\eta$($y_j$ - $\\hat{y}_j$)$x_i$\n",
    "     - $x_i$ ith input value for this instance\n",
    "     - $\\hat{y}_j$ out put of jth output neuron for this instance\n",
    "     - $y_j$ tartget output for jth neuron for this instance\n",
    "     - $\\eta$ learning rate\n",
    " - Only works for linear problems\n",
    " - Perceptrons do not output a class probability\n",
    " - Perceptrons cannot perform XOR operations\n",
    " - MLP - Multi-Layer Perceptrons can do XOR and other thins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42aaacbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b25aeda76ff5>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris.target == 0).astype(np.int) #1 for setosas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] #length/width of petal \n",
    "y = (iris.target == 0).astype(np.int) #1 for setosas\n",
    "\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0751a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74175",
   "metadata": {},
   "source": [
    " ## The Multilayer Perceptron and Backpropagation\n",
    "- MLPs are composed of an input layer, 1 or more hidden layers, and an output layer.\n",
    "- the input layer consists of pass through units and the other layers are TLUs\n",
    "- Rumelhart, Hinton, and Williams 1986 introduced backpropagation\n",
    "- **Backpropagation** - computes the gradient with respect to every single model parameter and for all the layers. Accomplishes this in two passes through the network. Finds how to tweak the weights in order to reduce the error. \n",
    "    - handles instances in minibatches\n",
    "    - cycles through whole dataset multiple times in epochs\n",
    "    - On the *forward pass* the instances are passed through the network and all the intermediate outputs are saved. \n",
    "    - error is measured by using a loss function which compares the actual output vs the desired output\n",
    "    - the chain rule is used to determine how much each output contributed to the error\n",
    "    - the algorithm works backwards, determining how much of these error contributions came from each connection in the next lower layer. It propagates the error gradient backwards through the network. \n",
    "    - gradient descent performed by tweaking all connections in the network using the error gradients just computed. \n",
    "- Step function is replaced by a sigmoid function so there is a gradient to follow. $\\sigma$ = $\\frac{1}{1 + e^{-z}}$\n",
    "    - this is an activation function like the hyperbolic tangent function or Rectified linear produce\n",
    "- non-linear activation functions allow the MLP to approximate non-linear continuous functions. \n",
    "\n",
    "## Regression MLPs\n",
    "- you need one output neuron per value you are trying to predict.\n",
    "- Usually you do not use an activation function for the output neurons. \n",
    "\n",
    "## Classification MLPs\n",
    "- can output the estimated probability for binary classification with a single output neuron\n",
    "- You need one output neuron per class you are predicting\n",
    "    - Softmax activation funciton will ensure all the outputs sum to one. This is useful for exclusive multiclass classification\n",
    "    - cross entropy loss function is useful here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311a53c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58797e37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1b556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
