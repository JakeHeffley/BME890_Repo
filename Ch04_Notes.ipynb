{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Notes: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    " - Chapter will cover various topics important for properly training an ML model\n",
    " - Will start with linear regression\n",
    "    - Closed form solution to directly calculate solution \n",
    "    - Gradient descent \n",
    "        - Batch GD, mini-batch GD, stochastic GD\n",
    " - Polynomial Regression\n",
    "    - Detect overfitting with learning curves\n",
    " - Regularization techniques for avoiding overfitting\n",
    " - Logistic regression\n",
    " - Softmax regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    " - notation and intro\n",
    "     - Linear regression is based off of the equation for a line: y =mx +b\n",
    "     - That equation can be transformed to yhat = theta0+theta1(x1) + thetan(xn)\n",
    "        - Yhat is the prediction\n",
    "        - Theta of 0 is the bias (y intercept)\n",
    "        - X 1 to n are the feature values \n",
    "        - Theta 1 to n are the weights for each feature value (aka parameters)\n",
    "        - So linear regression is sort of like a weighted sum with an offset\n",
    "    - Can also be written as yhat= theta * x \n",
    "        - Theta is the parameter vector\n",
    "    - Assuming MSE is our cost function then we will need to find the values for theta which minimize the MSE\n",
    "        - MSE(X,htheta) = (1/m) sum theta.Txi - yi)2   for each value of i from 1 to m\n",
    " - Normal Equation\n",
    "     - Normal equation is simply an equation that directly gives the values of theta which minimize the cost function \n",
    "     - thetaHat = (XTX)-1 * XTy\n",
    "        - thetaHat is the theta values that minimize the cost function\n",
    "        - y  is the vector of target values for instances 1 to m\n",
    " - Computational Complexity\n",
    "     - Compute requirements increase exponentially with the number of features n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    " - Intro\n",
    "    - GD is an optimization algorithm which finds the parameters which minimize a cost function. \n",
    "    - Measure the gradient of the error function function with regards to the parameter vector (theta) \n",
    "    - Random Initialization - start with random values for theta\n",
    "    - Learning Rate - the hyperparameter which determines the size of the change to theta after each step in GD. \n",
    "        - step size is proportional to the slope of the cost function. \n",
    "    - Step sizes that are too small will take too long to converge\n",
    "    - Step sizes too large will fail to converge\n",
    "    - Need to be careful about discerning global vs local minima in the gradient of theh cost function \n",
    "        - not a concer with MSE for linear regressino since it is always convex\n",
    "    - Parameter Space - space searched by GD for a minima. Its dimensionality is determined by the # of features. \n",
    " - Batch Gradient Descent\n",
    "    - GD involves calculating the derivative of the cost function with respect to each parameter (j) in theta\n",
    "    - Batch GD requires calculating the gradient over the full dataset, X. \n",
    "    - The derivative gives you a gradient vector which points uphill. So subtract that value from theta. Theta - deltaMSE(theta)\n",
    "    - Also include a learning rate multiplier, nu. \n",
    "    - Tolerance - A threshold for the gradient vector such that when the gradient is < the tolerance the algorithm will cease. Allows you to set the number of iterations very high so you know you converged on a minimum gradient value. \n",
    "    - Convergance Rate scales linearly and inversly with the tolerance. \n",
    " - Stochastic Gradient Descent\n",
    "    - randomly picks two instances and computes the gradient of the error function between them instead of using all the points in the dataset to calculate the error function. \n",
    "    - Able to take steps much faster with less compute. But will require more steps to approach the minimum and will never settle on the minimum. \n",
    "    - can be used as an out of core algo for huge datasets\n",
    "    - good at escaping local minima\n",
    "    - works well with a gradually decreasing local minima\n",
    "    - very sensitive to poorly shuffled data\n",
    "    - SGDRegressor()\n",
    " - Mini-Batch Gradient Descent\n",
    "     - computes the gradient on small mini batches\n",
    "     - its path through parameter space is more direct than SGD but more erratic than batch GD. \n",
    "     - takes less time on each step than batch GD.\n",
    "     - can utilize GPUs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    " - You can use linear models to fit nonlinear data using quadratic equations\n",
    " - $y = ax^2 + bx + c$\n",
    " - generate a non linear dataset with X and y\n",
    " - add the second degree polynomial to the training set as a new feature\n",
    "    - now instead of using $X_i$ to predict $y_i$ you use $X_i$ and $X^2_{i}$\n",
    "    - they used polynomialFeatures() from sklearn.preprocessing to generate the second degree poly for each instance of X\n",
    "    - fit a linear regressor and it will return an intercept and a coefficient for each feature ($X^2$ and X)\n",
    "    - if you started with more than one original feature then polynomialFeatures will calculate combinations of features at each polynomial degree  (a,b) becomes ($a,a^2,b,b^2,ab,a^2b,ab^2$) \n",
    "        - Note the exponential increase in features \n",
    "\n",
    "## Learning Curves\n",
    " - using multiple polynomials is an easy way to generate an overly complex model that will overfit the training data and fail to generalize. \n",
    " - Learning Curve - a plot of the models performance on the training set and the validation as a function of the training set size or training step. Basically, train the model on n instances of training/validation data and plot its error. Then do the same for n+1 isntances etc. \n",
    " - They give the example of a linear regression model estimating the nonlinear dataset. It underfits the data and you can tell because the RMSE of the training data set is less than the RMSE of the training and validation sets plateau with the trainRMSE very slightly < valRMSE\n",
    "    - adding more data to an underfit model will not improve performance \n",
    " - Next they plot the learning curve for a 10th degree polynomial regressor. \n",
    "    - the RMSE is significantly lower compared to the LR model.\n",
    "    - there is a gap between the trainRMSE and valRMSE with the valRMSE larger. This means the model is overfitting. This can be lessened by feeding the model much more data to train on. \n",
    " - The Bias/Variance Trade-off\n",
    "    - Bias - results from making faulty assumptions about the data. Often occurs because the model is too simple. \n",
    "    - Variance - results from excessive sensitivity to changes in the training data. Often occurs because the model is too complex or overfitting the data. \n",
    "    - Irreducible Error - due to noise in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    " - Intro \n",
    "    - $L_2$ norm of a vector is the square root of the sum of the square of each value in the vector. Or each dimension in the vector.  L2 norm = $sqrt(theta_i^2)$ where i = 1 to number of dimensions in the vector. \n",
    "    - $L_1$ norm of a vector is the sum of the absolute values of each dimension. \n",
    "    - Regularization is constraining a model to prevent over fitting\n",
    "        - e.g. reducing the number of degrees in a polynomial regressor\n",
    "        - linear models are often regularized by constraining the weights\n",
    " - Ridge Regression\n",
    "    - Regularization Term - the mathematical term used to implement regularization. In RR it is $alpha(1/2)(ss(theta_i))$, for all values of i=1:n,  which is added to the cost functtion. \n",
    "    - note that reg term only added to $theta_{i=1:n}$. It is not added to the bias term $theta_0$\n",
    "    - Regularization term should only be added to the cost function during training.\n",
    "    - Alpha ranges from 0 to 1. \n",
    "        - if alpha =1 then weights end up close to zero and the model is a horizontal line \n",
    "        - if alpha =0 then RR is just linear regression\n",
    "    - $\\textbf{w}$ = weights = theta_{1:n}\n",
    "    - reg term = $\\frac{1}{2}$ $||\\textbf{w}||_2^2$  where $||\\textbf{w}||_2$ is the $L_2$ norm (basically just the sum of squares)\n",
    "    - they show some examples in fig 4-17: \n",
    "        - for LR the reg term simply makes the line closer to horizontal\n",
    "        - for PR the reg term flatter curves \n",
    "    - RR can also be calculated with a closed form equation: $theta^{hat}$ = ($\\textbf{X}^T$$\\textbf{X}$ + alpha$\\textbf{A}$)$^{-1}$ $\\textbf{X}$ y   where A is the identity matrix except top left =0\n",
    " - Lasso Regression\n",
    "    - Least Absolute Shrinkage and Selection Operator Regression\n",
    "    - adds the $L_1$ norm of the weight vector to the cost function.\n",
    "    - tends to reduce the weights of the least important vectors to 0\n",
    "        - for PR this will reduce most of the higher degree polynomials to 0\n",
    "        - sort of an automatic feature selection to generate a more sparse model\n",
    "        - this means that lasso will travl down the gradient by reaching a space where on feature's gradient =0 then continue down the gradient of a different feature. (see fig4-19)\n",
    " - Elastic Net\n",
    "    - a mix of RR and Lasso\n",
    "    - uses a mix ratio, r, which controls the balance of ridge vs lasso regression\n",
    "    - MSE(theta) + r(alpha)(RR cost function) + $\\frac{1-r}{2}$(alpha)(lasso cost function)\n",
    "    - generally you want to use some form of regularization instead of plain LR. \n",
    "        - if you suspect that only a few features are important than use Lasso or elastic net. \n",
    " - Early Stopping\n",
    "    - good way to regularize any iterative learning algorithm\n",
    "    - Stops training when the validation error reaches a minimum. \n",
    "    - As a model trains its validation error will decrease. But then as it becomes overfit the validation error will increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    " - Intro \n",
    "    - logistic regression determines the probability that an instance belongs to a particular class. \n",
    " - Estimating Probabilities\n",
    "    - computes a weighted sum of the input features plus a bias term, just like linear regression\n",
    "    - vector form of estimated probability: p$^{hat}$ = $h_{theta}$($\\textbf{x}$) = sigma($\\textbf{x}^T$theta)\n",
    "    - sigma() represents a sigmoid function bounded by 0 and 1: sigma(t) = $\\frac{1}{1+e^{-t}}$\n",
    "    - if $p^{hat}$ for a given instance is > 0.5 then $y^{hat}$ for that instance is 1\n",
    " -  Training and the Cost Function\n",
    "    - objective is to set high prob. for positive instances and low prob. for negative instances (binary classifier)\n",
    "    - cost function for a given instance: c(theta) = -log($p^{hat}$) if y=1   and   -log(1-$p^{hat}$) if p=0\n",
    "    - different cost functions for each outcome. So if the prediction matches the outcome the cost will be low and vice versa. \n",
    "    - log loss - the average cost function over the training set: J(theta)= -$\\frac{1}{m}$$ \\sum_{i=1}^{m}y^ilog(p^{hat(i)})+(1-y^i)log(1-p^{hat(i)}) $\n",
    "    - no known cost function\n",
    "    - guaranteed global minimum since the gradient is convex. \n",
    " - Decision Boundaries\n",
    "    - logistic regression produces a decision boundary which is the point where the probability of a positive class ($p^{hat}$ crosses 50%\n",
    "    - if you have a single feature then the decision boundary is a point on a line, if you have two features it is a line in a plane, etc\n",
    "    - can use $L_2$ or $L_1$ for regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    " - Classification for multiple classes\n",
    " - computes the scores, $s_k$($\\textbf{x}$), for each class k for each instance $\\textbf{x}$\n",
    " -  $s_k$($\\textbf{x}$) = $\\textbf{x}^Ttheta^{(k)}$\n",
    "     - theta$^{(k)}$ is the parameter vector for each class.\n",
    " - Softmax Function\n",
    "     - calculate the probability $\\hat{p}_k$ that a given instance belongs to class k\n",
    "     - take the exponential of each score and normalizes them. \n",
    " - picks the class with the highest score for a given instance\n",
    " - predicts only once class at a time\n",
    " - Training \n",
    "    - Cross Entropy\n",
    "        - penalizes the model when it estimates a low probability for the target class\n",
    "        - measures the average number of bits you send per option\n",
    "    - find the gradient vector for each class. Thus settling on a parameter matrix which minimizes the cost function.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
